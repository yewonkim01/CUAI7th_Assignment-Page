from datetime import datetime
QA = {
    "ch01": {
        "chapter": "ch01",
        "chapter_name": "차원축소",
        "Q": [
                "Q. 신경망을 이용하여 분류 문제를 해결할 때 회귀문제를 해결할 때와 어떻게 달라지나요?",
                "Q. 두 번째 문제",
                "Q. 세 번째 문제",
                "Q. 네 번째 문제",
                "Q. 다섯 번째 문제"
            ],
        "A": [
                """출력층 부분이 달라진다.<br>
                분류 문제를 풀 경우에는 출력층의 활성화 함수를 softmax로 그리고 노드의 개수는 분류하는 class의 개수로 설정해야한다.<br>
                회귀 문제를 풀 경우에는 출력층의 활성화 함수를 항등함수로 그리고 노드의 개수는 한 개로 설정해야한다.""",
                """가나다""",
                """가나다""",
                """가나다""",
                """가나다""",
            ]
    },
    
    "ch02": {
        "chapter": "ch02",
        "chapter_name": "사이킷런으로 시작하는 머신러닝",
        "Q": [
                "Q. 지도학습이란 무엇인가요? 붓꽃 품종 예측하기를 예시를 들어 설명해주세요.",
                "Q. 학습 데이터와 검증 데이터, 테스트 데이터를 분리하는 이유는 무엇일까요?",
                "Q. k 폴드 교차검증이란 무엇인가요?",
                "Q. 레이블 인코딩과 원 핫 인코딩의 차이점은 무엇인가요?",
                "Q. 피처 스케일링이란 무엇이고 사용시 주의해야할 점은 무엇인가요?",
                "Q. 현재까지 공부한 사이킷런의 fit() 메소드의 두가지 사용 대해서 설명해 주세요.",
                "Q. 예측 성능 평가를 정확도로 했을때 사용할 수 있는 메소드는 무엇인가요?",
                "Q. 타이타닉에서 사용한 데이터 전처리를 두가지만 말씀해주세요."
            ],
        "A": [
                """지도학습이란 학습을 위한 다양한 피처와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의
테스트 데이터 세트에서 미지의 레이블을 예측합니다. 즉 지도학습은 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식입니다.<br>
<br>
기계학습은 거시적으로 지도학습과 비지도학습으로 나뉩니다. 두개의 핵심적인 차이는 학습하는
데이터의 구성이 서로 다르다는 점입니다.<br>
지도학습의 알고리즘들은 정답(label)이 있는 데이터들을 학습합니다. 그리고 이런 정답들은 annotation(표기)된것입니다. <br>
책에서 나온 '붓꽃 품종 예측하기' 같은 경우, 데이터의 피처인 sepal length, sepal width, petal length, petal width의 값들과 정답 레이블을
가진 train set으로 훈련합니다. 이후 레이블이 없는 test set의 레이블을 예측합니다. <br>
지도학습의 대표적인 예는 분류, 회귀가 있습니다. 붓꽃 품종 예측하기의 경우 분류에 해당합니다.<br>
반면 비지도학습은 label이 없는 train set 데이터를 학습하여 데이터 속의 패턴이나 데이터 간의 유사도를 찾아냅니다.<br>
대표적인 예는 군집화, 차원축소가 있습니다.""",
                """먼저 학습데이터와 테스트 데이터를 분리하지 않고 같은 데이터로 테스트를 한다면 이미 답 (label) 을 알고있는 문제를 
푸는것과 같습니다. 따라서 예측을 수행하는 데이터 세트는 학습에 사용한 데이터가 아닌 테스트 전용의 데이터여야 합니다.
사이킷런에서는 train_test_split() 메소드를 통해서 원본 데이터를 학습 데이터와 테스트 데이터로 나눌 수 있습니다.

하지만 이렇게 학습데이터와 테스트데이터를 분리했을때 머신러닝 모델이 학습데이터에만 치중되고 테스트 데이터의 정답 예측 성능이 떨어지는 과적합 문제가 발생할 수 있습니다.
학습 데이터가 편중되는것을 막기 위해 학습과정에서 검증데이터를 사용해서 교차검증을 합니다. 
""",
                """교차 검증을 하기 위해 데이터를 k개의 데이터 폴드 세트를 만들어서 k번만큼 각 폴드 세트에 학습과 검증 평가를 반복적으로 수행하는 방법입니다.<br>
예를 들어서 k가 5라면 5개의 폴드된 데이터 중 하나를 검증 데이터 셋으로 설정하고 나머지를 학습데이터로 사용하는것을 반복합니다.<br>
<br>
Stratified K 폴드는 K 폴드 교차검증시 K개의 데이터 폴드로 나누었을때 레이블의 분포를 고려하지 못하는 문제를 해결해줍니다.<br>
<br>
사이킷런에서 교차검증을 수행할 수 있는 API는 cross_val_score() 입니다.
""",
                """레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환합니다. 예를 들어서 
데이터의 피처가 상품이고 값이 TV, 냉장고, 전자레인지 등 문자열로 되어있을때, 머신러닝 학습을 위해서
문자열을 0,1,2 와 같은 숫자로 바꿔줄 수 있습니다. <br>
하지만 선형회귀와 같은 알고리즘을 사용한다면 숫자의 ordinal한 특성이 반영되어 독립적인 관측값간의 관계성이 생깁니다.<br>
예를들어서 1+2=3 과 같은 숫자의 특성은 TV + 냉장고 = 전자레인지의 잘못된 관계를 만듭니다. 또 TV < 냉장고 라는 잘못된 관계를 만듭니다.<br>
원 핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식입니다.
""",
                """피처스케일링이란 서로 다른 피처의 값들의 범위를 일정한 수준으로 맞추는 전처리 과정입니다.<br>
이 전처리를 해주는 이유는 서로 다른 피처간에 범위나 분포가 달라서 특정 피처에 편향되는것을 방지 하기 위함입니다.<br>
예를 들어서 성별 피처는 0 , 1 의 값을 가지고 나이는 10, 20, 30 값을 가지지만 성별이 더 큰 영향을 끼칠 수 있기 때문에 피처 스케일링을 해줍니다.<br>
<br>
피처 스케일링에는 대표적으로 표준화와 정규화가 있습니다. <br>
표준화는 데이터의 피처 각각이 평균 0 분산 1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미합니다.<br>
정규화는 데이터의 값을 모두 0~1 의 값으로 변환하는 것 입니다.<br>
<br>
사이킷런에서 표준화 클래스인 StandardScaler가 있습니다. <br>
또한 피처의 데이터 값을 특정 범위 값으로 변환해주는 MinMaxScaler가 있습니다.<br>
<br>
일반적으로 정규화의 경우 극단 값과 같은 특이 치의 영향을 크게 받기 때문에 이런 값이 있는 경우에는 표준화를 시키거나, 이상치를 제거 후 정규화를 시키는 것이 바람직 합니다.<br>
<br>
이러한 피처 스케일링시 가장 유의 해야 할 점은 <br>
test 데이터 셋을 fit() 메소드를 통해 변환 정보를 설정하지 않는 것 입니다.<br>
가능하다면 피처 스케일링 후 데이터 분리를 하거나, 그게 아니라면 train 데이터를 fit() 한 정보를 train, test 데이터에 transform() 시키는 방법이 바람직합니다.
""",
            """현재 공부한 사이킷런에서 사용한 fit() 메소드는 두가지 경우가 있습니다.<br>
<br>
첫번째는 머신러닝 모델을 학습하는 경우 입니다.<br>
두번쨰는 데이터 전처리 과정의 피처스케일링에서 사용한 것 입니다.<br>
<br>
사이킷런의 지도학습 머신러닝 모델은 fit() 을 사용하여 학습할 수 있습니다. <br>
이 지도학습 모델들인 Classifier 과 Regressor 클래스들을 합쳐서 Estimator 클래스 라고 부릅니다. <br>
공룡책 93페이지에 나와있는 ML알고리즘들을 sklearn API 홈페이지에 검색해 보면 Estimator 클래스들에 fit() 함수가 구현되어있는것을 확인할 수 있습니다.<br>
<br>
데이터 전처리 과정에서 사용한 fit() 메소드는 StandardScaler, MinMaxScaler 클래스에 구현되어있습니다.
""",
            """accuracy_score()""",
            """결측치 제거 : 결측치들을 평균또는 고정값으로 변경<br>
age 카테고리 할당<br>
레이블 인코딩<br>
불필요한 피처 제거"""
            ],
        "deadline": datetime(2024,3,11,23,59,59)
    },
    
    "ch03": {
        "chapter": "ch03",
        "chapter_name": "평가",
        "Q": [
                "Q. 정확도(accuracy)가 무엇인지 설명하고, 불균형한 데이터셋에서 정확도만으로 모델의 성능을 평가하기 어려운 이유는 무엇인지 오차행렬(confusion matrix)를 바탕으로 설명하시오.",
                "Q. 정밀도와 재현율이 무엇인지 설명하고, 각각의 지표가 상대적으로 더 중요한 지표인 상황은 무엇이 있는지 설명하시오.",
                "Q. 정밀도와 재현율 사이에 트레이드오프(Trade-off)가 발생하는 이유에 대해 설명하시오.",
                "Q. 분류 결정 임계값을 낮추면 정밀도와 재현율의 값이 어떻게 변할지 설명하고 그 이유에 대해 설명하시오.",
                "Q. F1 스코어가 무엇인지 설명하고, 해당 지표를 사용할때의 장점에 대해 설명하시오.",
                "Q. ROC 곡선과 AUC 스코어가 무엇인지 설명하시오.",
                "Q. 피마 인디언 당뇨병 예측에서 데이터 값이 0인 데이터을 삭제하지 않고 각 피처들의 평균 값으로 대체한 이유에 대해 설명하시오.",
            ],
        "A": [
                """정확도란 실제 데이터에서 예측 데이터가 얼마나 같은지를 판단하는 지표이다.<br>
오차 행렬에서 true에 해당하는 값인 TN과 TP에 좌우된다.<br>
정확도 = (TN + TP)/(TN + FP + FN + TP)<br>
<br>
따라서 불균형한 데이터 셋에서 Positive 혹은 Negative 둘 중 하나에 대한 예측 정확도만으로도 분류의 정확도가 매우 높게 나타나는 수치적인 판단 오류를 일으켜 모델의 신뢰도가 떨어지는 상황이 발생할 수 있다.""",
                """정밀도는 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율을 말한다. (TP / (FP + TP))<br>
재현율은 실제 값이 Positive인 대상 중에 예측 값과 실제 값이 Positive로 일치한 데이터의 비율을 뜻한다. (TP / (FN + TP))<br>
<br>
정밀도가 상대적으로 중요한 지표인 경우는 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우이다.<br>
재현율이 상대적으로 중요한 지표인 경우는 실제 Positive 양성인 데이터 예측을 Negative 음성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우이다.""",
                """정밀도와 재현율 모두 TP를 높이는 데 동일하게 초점을 맞추지만. <br>
정밀도는 FP를 낮추는 데, 재현율은 FN을 낮추는 데 초점을 맞춘다. <br>
<br>
따라서 서로 보완적인 지표로 어느 한 쪽을 강제로 높이면 다른 하나의 수치는 떨어지기 쉽다.""",
                """임계값을 낮추면 재현율 값이 올라가고 정밀도는 떨어진다.<br>
<br>
분류 결정 임계값은 Positive 예측값을 결정하는 확률의 기준이다.<br>
임계값을 낮출수록 True 값이 많아져 Positive 예측값이 많아지므로 상대적으로 재현율 값이 높아진다. <br>
양성 예측을 많이 하다 보니 실제 양성을 음성으로 예측하는 횟수가 상대적으로 줄어들기 때문이다.
""",
                """F1 score = 2 * [(정밀도 * 재현율) / (정밀도 - 재현율)]<br>
<br>
F1 스코어란 정밀도와 재현율을 결합한 지표이다. 정밀도와 재현율이 어느 한 쪽으로 치우치치 않는 수치를 나타낼 때 상대적으로 높은 값을 가지므로 분류의 종합적인 성능 평가에 사용될 수 있다.""",
                """ROC곡선이란 FPR(False Positive Rate)이 변할 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선이다. <br>
이때, TPR(민감도)은 실제값 Positive가 정확히 예측돼야 하는 수준을 나타내며 TNR(특이성)은 실제값 Negative가 정확히 예측돼야 하는 수준을 나타낸다. <br>
<br>
AUC스코어란 ROC 곡선 밑의 면적을 구한 것으로서 일반적으로 1에 가까울수록 좋은 수치이다. <br>
FPR이 작은 상태에서 얼마나 큰 TPR을 얻을 수 있냐에 따라 AUC수치가 결정된다.
""",
                """전체 데이터 건수가 많지 않기 때문에 이들 데이터를 일괄적으로 삭제할 경우에는 학습을 효과적으로 수행하기 어렵기 때문에 평균 값으로 대체했다.""",
            ]
    },
    
    "ch04-1": {
        "chapter": "ch04-1",
        "chapter_name": "분류 Part.1",
        "Q": [
                "Q. 분류가 무엇인지 설명한 뒤, 새로운 데이터 값이 들어왔을 때 분류 방식을 사용할때의 상황에 대해 설명하시오.",
                "Q. 의사결정트리는 어떤 노드들로 구성되어있으며, 각각의 노드에 대해 서술하시오.",
                "Q. 의사결정트리란 무엇이며, 장단점에 대해 서술하시오.",
                "Q. 의사결정트리에서 과적합을 방지하기 위한 각 파라미터에 대해 서술하시오.",
                "Q. 앙상블 방식 학습에 대해 설명하고, 어떤 종류의 방식이 있는지 서술하시오.",
                "Q. 보팅 방식의 경우, 어떤 종류가 있으며 각각의 방식에 대해 설명하시오.",
                "Q. 배깅 방식의 대표 알고리즘과, 어떤 장점이 있는지 서술하시오.",
                "Q. 부스팅 방식이란 무엇인지 서술한 뒤, 3가지 예시를 제시하시오.",
                "Q. XGBoost의 장점 3가지를 서술하시오."
            ],
        "A": [
                """분류는 학습 데이터로 주어진 데이터의 피처와 레이블 값을 학습해, 새로운 값이 들어오게 되면 미지의 레이블 값을 예측하는 것이다. <br>
<br>
따라서, 새로운 데이터 값이 들어왔다면 미지의 레이블 값을 예측하는 형식으로 동작할 수 있을 것이다.""",
                """의사결정트리는 규칙노드(Decision Node)와 리프노드(Leaf Node)로 구성된다. 이때 규칙노드는 하위 트리를 분할하는 규칙을 가지는 것이로, 리프 노드는 결정 값을 가진다.""",
                """의사결정트리는 쉽고 유연하게 적용할 수 있는 알고리즘이다. <br>
<br>
이는 데이터 스케일링, 정규화 등의 사전 가공의 영향이 적다는 장점이 있고, 이에 반해 예측 성능을 향상시키기 위한 복잡한 규칙 구조로 인한 과적합이 발생할 수 있다는 단점 또한 존재한다.""",
                """* 노드 분할 위한 최소 샘플 데이터수(min_samples_split):작을수록 분할되는 노드가 많아져 과적합 가능성 증가<br>
* 말단 노드가 되기 위한 최소 샘플 데이터 수(min_samples_leaf): 작을수록 말단 노드 증가로 과적합 가능성 증가(단,레이블 데이터 분포가 비대칭이면 작게 설정 필요)<br>
* 최적의 분할까지 고려할 최대 피처개수(max_features)<br>
* 트리의 최대깊이(max_depth):깊이가 깊어질수록 과적합 가능성이 증가하는 파라미터이다.<br>
* 말단노드의 최대개수(max_leaf_nodes)""",
                """앙상블 방식의 학습이란, 약한 학습기(예측 성능이 떨어지는 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 업데이트하면서 예측 성능을 향상시키는 기법이다.<br>
해당 방식의 종류에는 보팅, 배깅, 부스팅 방식이 존재한다.""",
                """보팅에는 하드보팅과 소프트보팅 방식이 존재한다. 하드보팅 방식은 다수결의 원칙을 따르는 것과 같고, 소프트보팅 방식은 클래스별 각각의 확률을 평균하여 결정하는 방식이다.""",
                """배깅 방식의 대표 알고리즘은 랜덤포레스트가 존재한다. 배깅방식의 경우 빠른 수행속도를 보이며, 다양한 영역에서 높은 예측 성능을 보인다는 장점이 존재한다.""",
                """부스팅 방식은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다. 해당 방식의 예로는 GBM, XGBoost, LightGBM이 있다.""",
                """1. 뛰어난 예측 성능<br>
2. GBM 대비 빠른 수행 시간<br>
3. CPU 병렬 처리, GPU 지원<br>
4. 다양한 성능 향상 기능<br>
5. 규제 기능<br>
6. Tree Pruning<br>
7. 다양한 편의 기능<br>
8. Early Stopping<br>
9. 자체 내장된 교차 검증<br>
10. 결손값 자체 처리""",
            ]
    },
    
    "ch04-2": {
        "chapter": "ch04-2",
        "chapter_name": "분류 Part.2",
        "Q": [
                "Q. LightGBM의 장점과 단점을 각각 한 가지 이상 서술하시오. (기존의 알고리즘과 비교)",
                "Q. LightGBM에서 사용하는 트리의 depth가 상대적으로 깊은 이유에 대해 서술하시오.",
                "Q. LighGBM에서 사용하는 리프 중심 트리 분할(Leaf Wise) 방식에 대해 설명하시오.",
                "Q. 파이썬에서 LightGBM 클래스를 사용하려고 합니다. 여기서 과적합(overfitting)을 해결하기 위해 사용하는 하이퍼 파라미터들 중에서 2가지만 적고, 가볍게 설명하시오.",
                "Q. Grid Search의 장점과 단점을 각각 서술하시오.",
                "Q. 베이지안 최적화 개념의 큰 흐름을 서술하시오.",
                "Q. 오버 샘플링에 대해 설명하고, 오버 샘플링을 위해 데이터를 단순히 증식하면 안되는 이유에 대해 서술하시오.",
                "Q. 스태킹 앙상블 모델 개념의 큰 흐름에 대해 서술하시오.",
            ],
        "A": [
                """(장점)<br>
1. 기존 XGBoost에 비해 "학습"에 걸리는 시간이 훨씬 적다.<br>
2. 기존 XGBoost에 비해 "예측"에 걸리는 시간이 훨씬 적다.<br>
3. 메모리 사용량도 다른 알고리즘에 비해 상대적으로 적다.<br>
4. XGBoost와 예측 성능에는 차이가 없고, 기능상의 다양성은 오히려 크다.<br>
<br>
(단점)<br>
1. 일반적으로 10,000건 이하의 데이터 세트를 사용할 경우, 과적합이 발생하기 쉽다.""",
                """일반 GBM 계열과 다르게, LightGBM은 Leaf wise 기반을 사용하므로 깊이가 상대적으로 더 깊다.""",
                """LightGBM의 리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고, 최대 손실 값(max delta loss)을 갖는 리프 노드를 지속적으로 분할하는 방식이다.""",
                """과적합을 막기 위해서는 모델의 복잡도를 줄여야한다.<br>
모델의 복잡도를 줄일 수 있는 하이퍼 파라미터는 다음과 같다.<br>
<br>
1. num_leaves: 개별 트리가 가질 수 있는 최대 리프의 개수, 이 개수를 낮게 지정하면 모델의 복잡도가 줄어든다.<br>
2. min_data_in_leaf(or min_child_samples): 최종 결정 클래스인 리프 노드가 되기 위해서 최소한으로 필요한 레코드의 수, 이를 큰 값으로 설정하면 모델의 복잡도가 낮아진다.<br>
3. max_depth: 명시적으로 깊이의 크기를 제한, 이를 줄이면 모델의 복잡도가 낮아진다.<br>
<br>
이 외에도 learning_rate와 num_iterations(or n_estimators)를 조정하거나, lambda_l2(or reg_lambda) 또는 lambda_l2(or reg_alpha)를 조정하여 모델의 복잡도를 낮출 수 있다.""",
                """(장점) 내가 원하는 범위를 정확하게 비교 분석 가능하게 한다.<br>
<br>
(단점) XGBoost나 LightGBM같이 하이퍼 파라미터 개수가 많은 알고리즘에서는 시간이 매우 오래걸린다는 단점이 있다.""",
                """베이지안 최적화는 이름에서도 알 수 있듯이 베이지안 확률에 기반을 두고 있다. <br>
베이지안 확률에서는 사전 확률을 새로운 데이터를 이용하여, 사후 확률로 업데이트를 한다.<br>
베이지안 최적화에서도 이와 비슷한 흐름으로 최적의 하이퍼 파라미터 조합을 찾아낸다.<br>
<br>
우선, 대체 모델(Surrogate Model)이 획득 함수(Acquisition Function)로부터 최적 함수를 예측할 수 있는 입력값(여기서는 하이퍼 파라미터)을 추천 받은 뒤, 이를 기반으로 최적 함수 모델을 개선한다. 획득 함수는 개선된 대체 모델을 기반으로 최적 입력값을 다시 계산한다. 이 과정을 반복한다. (계속 업데이트하는 과정이 베이지안 확률의 큰 흐름과 비슷한 것을 알 수 있다.)""",
                """오버 샘플링은 지도 학습에서 극도로 불균형한 레이블 값 분포를 갖는 학습 데이터 문제를 해결하기 위한 방법으로, 데이터를 증식시킴으로써 불균형을 해결하는 방법이다.<br>
오버 샘플링은 적은 레이블을 가진 데이터 세트를 많은 레이블을 가진 데이터 세트 수준으로 증식시킨다.<br>
여기서 데이터를 증식시킬 때, 단순히 증식(단순 복제)을 하면 안되는 이유는 과적합(overfitting) 문제가 발생할 수 있기 때문이다.""",
                """성능이 비슷한 모델을 각각 모델별로 학습을 시킨 뒤, 예측을 수행한다. 이후 이 결과를 다시 합쳐(스태킹) 새로운 데이터 세트를 만든다. 그 후 최종 모델에 앞서 만든 데이터 세트를 통해 훈련을 하고 최종 예측을 수행한다.""",
            ]
    },
    
    "ch05": {
        "chapter": "ch05",
        "chapter_name": "회귀",
        "Q": [
                """Q. 회귀 분석의 유형은 크게 독립변수의 수, 선형성, 종속변수의 수로 구분할 수 있습니다.<br>
회귀 분석 유형 총 8가지를 작성해 주시고, 그중 자유롭게 한 가지 유형에 대해 모델링 해주세요.<br>
ex) 다중 독립변수, 단일 종속변수, 선형 회귀<br>
Y(집값) = W1X1(방 개수) + W2X2(방 크기) + W3*X3(학군)""",
                """Q. 단일 선형 회귀 모델에서는 회귀 계수를 정하기 위해 loss function을 RSS로 사용할 수 있습니다.<br>
RSS는 잔차(residual)의 제곱의 합으로 계산하는데,<br>
loss function으로 단순히 잔차(residual)의 합을 계산하는 것에 비해 RSS의 장점이 무엇인지 설명해 주세요.<br>
<br>
가능하다면 RSS가 가질 수 있는 한계점에 대해서도 생각해 보세요.""",
                """Q. loss function을 최소화하기 위해 주로 경사 하강법을 사용하는 이유와 <br>
                weight update 시 기존 weight에서 학습률x편미분 값을 '빼주는' 이유를 설명해 주세요.""",
                """Q. 철수는 집값을 예측하기 위해 다양한 선형 회귀 모델을 만들었습니다.<br>
하지만 아직 평가 지표를 정하지 못했습니다.<br>
테스트할 집값의 label 데이터가 다음과 같습니다.<br>
400,000원 250,000원 800,000원 29,342,0000원<br>
철수는 가능하면 아웃 라이어에 의해 평가 값의 변동이 크지 않았으면 합니다.<br>
이때 MAE , RMSE 중 어떤 평가 지표를 선택하면 좋을까요? 그 이유도 적어주세요.""",
                """Q. 다항회귀에서 degree 설정에 따른 모델의 복잡성과 bias-variance 에 관해 설명해주세요.<br>
<br>
또 가능하다면 모델에 따라서 bias와 variance가 높거나/낮아지는 이유와, bias와 variance가 trade-off 인 이유를 설명해주세요.""",
                """Q. L2 규제는 loss function의 값을 줄여나갈 때, alpha 값에 따라서 wight들의 L2 norm까지 최소화시켜야 합니다. alpha 값이 커진다면, 모델이 단순해지면서 오버 피팅을 개선합니다. <br>
반면 L1 규제는 weight의 절댓값을 최소화시킵니다. 이것은 영향력이 크지 않은 회귀계수 값을 0으로 만드는 피처 셀렉션의 역할을 합니다.<br>
<br>
그런데 여기서 L1 규제가 L2 규제와 비교했을 때, 불필요한 회귀계수 값을 0으로 만드는 이유는 무엇일까요?""",
                """Q. 로지스틱 회귀가 선형회귀와 다른 점은 무엇인가요?""",
                """Q. 회귀 트리와 분류 트리의 예측 결정값을 만드는 차이점에 대해서 설명해 주세요.""",
            ],
        "A": [
                """단일 선형 회귀분석<br>
단일 비선형 회귀분석<br>
단일 선형 다변량 회귀분석<br>
단일 비선형 다변량 회귀분석<br>
다중 선형 회귀분석<br>
다중 비선형 회귀분석<br>
다중 선형 다변량 회귀분석<br>
다중 비선형 다변량 회귀분석<br>
단순 선형 회귀분석 : Y(집값) = W1X1(방 개수) + W2X2(방 크기) + W3*X3(학군)""",
                """잔차의 합으로 loss function을 계산하게 되면, 각각의 오류가 큼에도 불구하고<br>
부호 때문에 오류의 합이 줄어들 수 있습니다. 또한 미분 시에도 RSS가 편리합니다.<br>
<br>
그럼에도 RSS는 vertical error를 구하기 때문에 Euclidean 거리보다 정확하지 않습니다.<br>
따라서 aX + bY = c , normal vector ( a , b ) 와 같은 line으로 모델링하고<br>
loss function = sum of ( ax + by - c ) 와 같은 형태로 정하는 방법이 있습니다.""",
                """최적의 parameter를 찾기 위해서 loss function을 미분하여 최대 최소를 구할 수도 있습니다.<br>
하지만 복잡하거나 비선형적인 함수 등에서 미분 후 최대/최소가 되는 weight를 단번에 찾기 힘든 경우가 많기 때문에 점진적으로 weight를 조정해 나가는 경사 하강법을 주로 사용합니다.<br>
weight update 시, 현재 weight에서 loss function가 감소 중이라면 minima는 현재 weight로부터 증가된 위치에 있을 것이고 loss function이 증가 중이라면 minima는 현재 weight로부터 감소된 위치에 있을 것입니다.<br>
그렇기 때문에 편미분 값을 빼준다면 weight는 그래프가 감소 중일 때 weight를 증가시키고 그래프가 증가 중일 때 weight를 감소시켜 줍니다.""",
                """RMSE.<br>
<br>
RMSE는 잔차를 제곱하고 다 더한 뒤 루트를 씌웠습니다. 따라서 MAE에 비해 큰 오류 값에 더 큰 페널티를 줍니다.<br>
따라서 RMSE를 사용하면 아웃 라이어에 의해 다른 잔차에 비해 훨씬 큰 잔차가 생기더라도, 평가 지표는 크게 변동 없는 값을 가질 것입니다.<br>
예를 들어서 400,000원 250,000원 800,000원에 대한 MAE는 400,000원 250,000원 800,000원 29,342,0000원에 대한 MAE와 큰 차이가 있는 반면 RMSE 값은 MAE에 비해 크게 변화가 없을 것입니다.""",
                """교재에서 예시를 들어준 다항회귀의 경우 degree=1로 설정하면 (즉 모델이 간단해지면) bias가 높아지고 variance가 낮아져서 언더 피팅이 생깁니다. <br>
하지만 degree= 15로 설정한다면 ( 모델이 복잡해지면 ) bias가 낮아지고 variance가 높아져서 오버 피팅이 생깁니다.<br>
이렇게 일반적으로 모델의 복잡성에 따라서 bias와 variance는 trade off 관계에 있습니다.<br>
<br>
그 이유는 다음과 같습니다.<br>
y = f(x) + noise 인 y 점들이 있고, f에 근사하는 f'을 찾으려고 합니다.  MSE는 다음과 같습니다.<br>
E[(y-f')^2] = variance of noise + variance of the f' + (bias of f')^2   ( 증명 생략 )<br>
이때, variance of noise 는 불변이며,<br>
variance of the f' 는 근사값의 분산<br>
(bias of f')^2 는 근사값의 평균이 f에 대해 치우친 정도입니다. <br>
따라서 복잡한 모델에서는 variance가 커지고 간단한 모델에서는 bias가 커집니다.<br>
이때 variance of the f'  와 (bias of f')^2 는 같은 항을 공유하기 때문에 trade off 관계에 있습니다.""",
                """L1 규제는 loss fucntion을 미분했을 때, new weight = old weight - lr * patial derivate - lr * alpha * sign(w) 와 같은 형태가 됩니다.<br>
즉, weight의 크기에 관계없이 +/- 부호만을 가지고 weight들을 update 하기 때문에 자잘한 weight들은 0으로 수렴해버리고 중요한 weight들만 남게 됩니다.<br>
그에 비해서 L2 규제는 loss function을 미분했을 때, new weight = old weight - lr * patial derivate - lr * alpha * (w) 와 같은 형태가 됩니다.<br>
즉, weight update 시 현재 weight 값을 사용하여 크기가 큰 weight는 강하게, 크기가 작은 weight는 약하게 규제를 가합니다.""",
                """학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정합니다.""",
                """분류 트리는 특정 클래스 레이블을 결정합니다.<br>
<br>
회귀 트리는 리프 노드에 속한 데이터 값의 평균값을 구해 회귀 예측값을 계산합니다.""",
            ]
    },
    
    "ch06": {
        "chapter": "ch06",
        "chapter_name": "차원축소",
        "Q": [
                "Q. 차원 축소를 하는 이유에 대해 설명하시오.",
                "Q. 차원 축소는 크게 피처 선택과 피처 추출로 나눌 수 있다. 각각의 특징에 대해 설명하시오.",
                "Q. PCA가 무엇인지 행렬의 관점에서 설명하고, PCA의 수행 과정에 대해 4단계로 설명하시오.",
                "Q. LDA와 PCA간의 차이점에 대해 설명하시오.",
                "Q. LDA의 수행 과정을 4가지 단계로 설명하시오. (수식을 상세하게 설명할 필요는 없습니다.)",
                "Q. SVD와 PCA의 차이점에 대해 설명하시오.",
                "Q. Truncated SVD가 무엇인지 설명하시오.",
                "Q. NMF에 대해 설명하시오.",
            ],
        "A": [
                """일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀이지게 되고 희소한 구조를 가진다. 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어진다. <br>
또한 피처가 많을 경우 개별 피처간에 상관관계가 높을 가능성이 크며 다중 공선성의 문제 등이 발생해 모델의 예측 성능이 저하될 가능성이 있다.<br>
<br>
따라서 차원 축소를 이용해 피처의 수를 줄이면 더 직관적으로 데이터를 해석할 수 있으며 학습 데이터의 크기가 줄어들어 학습에 필요한 처리 능력도 줄일 수 있다.""",
                """피처 선택은 특성을 선택하는 것으로 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것이다.<br>
<br>
피처 추출은 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것이다. 이렇게 새로 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 된다.""",
                """입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변환하는 방식을 PCA라고 한다.<br>
<br>
PCA의 수행 과정은 다음과 같다.<br>
1. 입력 데이터 세트의 공분산 행렬을 생성한다.<br>
2. 공분산 행렬의 고유벡터와 고유값을 계산한다.<br>
3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출한다.<br>
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다.""",
                """LDA는 선형 판별 분석법으로 PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법이지만, LDA는 지도학습의 분류에 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소한다.<br> 
<br>
PCA는 입력 데이터의 변동성의 가장 큰 축을 찾았지만, LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾는다.""",
                """1. 클래스 내부와 클래스 간 분산 행렬을 구한다. 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 백터를 기반으로 구한다.<br>
2. 클래스 내부 분산 행렬과 클래스 간 분산 행렬을 고유벡터로 분해한다.<br>
3. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼) 추출한다.<br>
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다.""",
                """PCA의 경우 정방행렬만을 고유벡터로 분해할 수 있다.<br>
SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있다. <br>
<br>
(책 427p)<br>
데이터 세트가 스케일링으로 데이터 중심이 동일해지면 SVD와 PCA는 동일한 변환을 수행한다. <br>
하지만 PCA는 밀집 행렬에 대한 변환만 가능하며 SVD는 희소 행렬에 대한 변환도 가능하다.""",
                """Truncated SVD는 Sigma 행렬에 있는 대각 원소, 즉 특이값 중 상위 일부 데이터만 추출해 분해하는 방식이다. <br>
이렇게 분해하면 인위적으로 더 작은 차원으로 분해하기 때문에 원본 행렬을 정확하게 다시 원복할 수는 없다. 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본 행렬을 근사할 수 있다.""",
                """NMF는 낮은 랭크를 통한 행렬 근사 방식의 변형이다. 원본 행렬 내의 모든 원소 값이 모두 양수라는 게 보장되면 좀 더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭한다.<br>
<br>
EX) V(4*6) = W(4*2) * H(2*6)""",
            ]
    },
    
    "ch07": {
        "chapter": "ch07",
        "chapter_name": "군집화",
        "Q": [
                "Q. K-평균 알고리즘이 무엇이며, 군집 생성과정을 설명하시오.",
                "Q. 군집화 중에서, 좋은 군집화를 하기 위한 조건을 서술하시오.",
                "Q. 평균이동이 무엇이며, 평균 이동에 대한 과정을 설명하시오.",
                "Q. 평균 이동의 장점과 단점에 대해 서술하시오",
                "Q. GMM과 K-Means 알고리즘을 비교하시오",
                "Q. DBSCAN에 대해 어떤 파라미터를 사용하는지 포함하여 설명하시오.",
                "Q. DBSCAN 군집화가 이뤄지는 과정을 설명하시오.",
                "Q. RFM 기법을 포함하여 고객 세그멘테이션에 대해 설명하시오.",
            ],
        "A": [
                """군집 중심점(Centroid)이라는 특정한 임의의 지점을 선택해 해당 중심에 가장 가까운 포인트들을 선택하는 군집화 기법이다.<br>
<br>
Step 1: 군집화의 기준이 되는 중심을 구성하려는 군집 개수만큼 정함<br>
Step 2: 각 데이터는 가장 가까운 곳에 위치한 중심점에 속함<br>
Step 3: 소속이 결정되면 군집 중심점을 소속된 데이터의 평균 중심으로 이동함<br>
Step 4: 기존에 속한 중심점보다 더 가까운 중심점이 있다면 해당 중심점으로 다시 소속 변경<br>
Step 5: 다시 중심을 소속된 데이터의 평균 중심으로 이동<br>
Step 6: 위 프로세스를 반복, 데이터의 중심점 변경이 없으면 반복 중단 및 군집화 종료""",
                """전체 실루엣 계수의 평균값(silhouette_score())이 0~1 사이 값을 가지며, 1에 가까움<br>
개별 군집의 평균값의 편차가 크지 않음(개별 군집의 실수엣 계수 평균값이 전체 실루엣 계수의 평균값에서 크게 벗어나지 않음)""",
                """중심을 군집의 중심(데이터가 모여 있는 밀도가 가장 높은 곳)으로 지속적으로 움직이면서 군집화를 수행한다. 이는, 군집 중심점이 데이터 포인트가 모여있는 곳이라는 생각에서 착안하였다.<br>
<br>
평균 이동에는 주로 확률 밀도 함수를 이용한다.<br>
<br>
평균 이동의 과정 <br>
주변 데이터와의 거리 값을 KDE 함수 값으로 입력한 뒤 그 반환 값을 현재 위치에서 업데이트하면서 이동한다.""",
                """**장점**<br>
데이터 세트의 형태 가정, 특정 분포도 기반의 모델 가정 필요 없음<br>
유연한 군집화 가능<br>
이상치의 영향력이 크지 않음<br>
미리 군집의 개수를 정하지 않음<br>
<br>
**단점**<br>
수행 시간이 오래 걸림
bandwidth의 크기에 따른 군집화 영향이 매우 큼""",
                """**K-means**<br>
평균 거리 기반으로 군집화<br>
원형의 범위에서 군집화 수행(데이터 세트가 원형의 범위를 가질수록 군집화 효율 증가)<br>
<br>
**GMM**<br>
보다 유연하게 다양한 데이터 세트에 적용""",
                """특정 공간 내에 데이터 밀도 차이에 기반한 알고리즘으로 군집화 수행하는 알고리즘이다. 이는, 복잡한 기하학적 분포도를 가진 데이터 세트에 대해서 군집화를 잘 수행한다.<br>
<br>
파라미터의 경우 다음과 같다.<br>
입실론 주변 영역(epsilon): 개별 데이터를 중심으로 입실론 반경을 가지는 원형의 영역<br>
최소 데이터 개수(min points): 개별 데이터의 입실론 주변 영역에 포함되는 타 데이터의 개수""",
                """Step 1: P1을 기준으로 이웃 포인트가 6개 이므로 P1은 핵심 포인트<br>
Step 2: P2를 기준으로 이웃 포인트 5개이므로 P2는 핵심 포인트<br>
Step 3: P1의 이웃 포인트 P2가 핵심 포인트이므로 직접 접근 가능<br>
Step 4: P1에서 직접 접근이 가능한 P2를 서로 연결하여 군집화 구성<br>
Step 5: 위 과정을 반복하여 점차적으로 군집 영역 확장""",
                """다양한 기준으로 고객을 분류하는 기법을 말한다. 이는 어떤 상품을 얼마나 많은 비용을 써서 얼마나 자주 사용하는가에 기반한 정보로 분류하는 것이 중요하다.<br>
<br>
주요 목표: 타깃 마케팅<br>
타깃 마케팅: 고객을 여러 특성에 맞게 세분화해서 그 유형에 따라 맞춤형 마케팅이나 서비스 세공<br>
<br>
**RFM 기법**<br>
RECENCY: 가장 최근 상품 구입 일에서 오늘까지의 기간<br>
FREQUENCY: 상품 구매 횟수<br>
MONETARY VALUE: 총 구매 금액""",
            ]
    },
# 
}